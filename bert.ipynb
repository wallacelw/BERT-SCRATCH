{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Pre-Training from scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can loggin to Hugging Face to interact with its hub\n",
    "\n",
    "- There, you can also save repositories, upload datasets, upload models and more\n",
    "\n",
    "- For now, I will save up everything locally"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check enabled GPU\n",
    "\n",
    "import torch\n",
    "\n",
    "# torch.zeros(1).cuda()\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dataset from wikipedia pages (2023) (English)\n",
    "\n",
    "- each document (page in wikipedia) is save in a row \n",
    "\n",
    "- the wikipedia dataset has several columns, including title and etc. I only used the main text of the page\n",
    "\n",
    "- I tried using BookCorpus dataset also. But it became too much for the training, so this is something to consider afterwards\n",
    "\n",
    "- Luckly, downloading dataset from hugging face only needs to be done once, and it will be saved in cache for future loads!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d827943fcd944ab6854660935c18d828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fb8a22fe7247b6a2c6e671cf30169e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "wikipedia = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
    "# bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "\n",
    "wikipedia = wikipedia.remove_columns([col for col in wikipedia.column_names if col != \"text\"])  # only keep the 'text' column\n",
    "\n",
    "#assert bookcorpus.features.type == wikipedia.features.type\n",
    "\n",
    "#raw_datasets = concatenate_datasets([bookcorpus, wikipedia])\n",
    "\n",
    "# def remove_non_ascii(example):\n",
    "    # example[\"text\"] = example[\"text\"].encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "# raw_datasets = wikipedia.map(remove_non_ascii)\n",
    "\n",
    "raw_datasets = wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wikipedia dataset has 6,4 M documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 6407814\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a tokenizer, starting from a pre-trained configuration of BERT from hugging face\n",
    "\n",
    "- It is important to train a tokenizer, because it is the responsible for representing the input data so the model is able to interact and interpret the vocabulary provided!\n",
    "\n",
    "- each word used in the input, should have a token (or a sequence of tokens) that will be used to represent it, and become embeddings when interacting with the model\n",
    "\n",
    "- the pre-loaded tokenizer is uncased, meaning that all uppercase letters will be converted to lowercase to reduce complexity and vocabulary size\n",
    "\n",
    "It will contain the following special tokens:\n",
    "\n",
    "    [UNK]: Unknown\n",
    "    [SEP]: Separator (for sequences)\n",
    "    [PAD]: Padding (to fill empty spots)\n",
    "    [CLS]: Classification (initial token used as classifier)\n",
    "    [MASK]: Masking (token that represents a masked token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using a batch iterator that will split the dataset into batches of size 10_000.\n",
    "\n",
    "And load the configurations of the pre configured **bert-base-uncased**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# create a python generator to dynamically load the data, one batch at a time\n",
    "def batch_iterator(batch_size=10_000):\n",
    "    for i in tqdm(range(0, len(raw_datasets), batch_size)):\n",
    "        yield raw_datasets[i : i + batch_size][\"text\"]\n",
    "\n",
    "# load a tokenizer from existing one to re-use special tokens\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-loaded tokenizer already has a vocabulary_size (unique tokens) of size 30522\n",
    "\n",
    "I will increase it to 35_000 so it can learn some more tokens from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 35_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the tokenizer: *(skipped, because this block of code was executed only once)*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# train tokenizor\n",
    "bert_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    text_iterator=batch_iterator(), \n",
    "    vocab_size=vocabulary_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tokenizer: *(skipped, because this block of code was executed only once)*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# save locally\n",
    "bert_tokenizer.save_pretrained(\"tokenizers/35_000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='tokenizers/35_000', vocab_size=35000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"tokenizers/35_000\", local_files_only=True)\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test tokenizer with a sample text \n",
    "\n",
    "As you can see, the input string will be splitted into several token (saved in the vocabulary) and each token has a unique ID\n",
    "\n",
    "Open: ***/tokenizers/35_000/vocab.txt*** to see more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 25281, 26019, 25281, 43, 25281, 25041, 43, 28445, 24988, 25281, 25281, 43, 25281, 35, 3]\n",
      "['[CLS]', 'can', 'you', 'can', 'a', 'can', 'as', 'a', 'cann', '##er', 'can', 'can', 'a', 'can', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sample = '''\n",
    "Can you can a can as a canner can can a can?\n",
    "'''\n",
    "\n",
    "encoding = tokenizer.encode(sample)\n",
    "\n",
    "print(encoding)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bert-base-uncased tokenizer was configured to use a {model_max_length} of 512, meaning that the sequence input given to the bert model can have up to 512 tokens (context size).\n",
    "\n",
    "Nevertheless, with a GTX 1070, the memory of the video card was not enough for this context size. \n",
    "\n",
    "Therefore, I will reduce it to 128 and truncate each document in the dataset to have upto 128 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard limit the size of context\n",
    "tokenizer.model_max_length = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step is to preprocess the dataset, tokenizing all the text, truncating documents with more than 128 tokens and saving it afterwards \n",
    "\n",
    "*(skipped, because this block of code was executed only once)*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "\n",
    "print(f\"The max length for the tokenizer is: {tokenizer.model_max_length}\")\n",
    "\n",
    "def group_texts(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "       examples[\"text\"], \n",
    "       return_special_tokens_mask=True, \n",
    "       truncation=True, \n",
    "       max_length=tokenizer.model_max_length\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "# preprocess dataset\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(group_texts, \n",
    "                                      batched=True, \n",
    "                                      remove_columns=[\"text\"], \n",
    "                                      num_proc=num_proc\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dataset, now, already tokenized, locally. *(skipped, because this block of code was executed only once)*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# save tokenized dataset locally:\n",
    "tokenized_datasets.save_to_disk(f\"dataset/tokenized-train/{tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And always load the tokenized dataset from the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'special_tokens_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "    num_rows: 6407814\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# load tokenized dataset locally\n",
    "tokenized_datasets = load_from_disk(f\"dataset/tokenized-train/{tokenizer.model_max_length}\")\n",
    "\n",
    "print(tokenized_datasets.features)\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The attention mask column will define it one token should be ignored in the attention layer of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test tokenizer:\n",
    "\n",
    "the prefix ***##*** means that this token is joined with the last token in the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ana', '##r', '##chi', '##s', '##m', 'is', 'a', 'political', 'philosophy', 'and', 'movement', 'that', 'is', 'sk', '##ep', '##tic', '##al', 'of', 'all', 'just', '##ifications', 'for', 'authority', 'and', 'seek', '##s', 'to', 'ab', '##olis', '##h', 'the', 'institutions', 'it', 'claims', 'maintain', 'un', '##ne', '##cess', '##ary', 'co', '##erc', '##ion', 'and', 'hier', '##arch', '##y', ',', 'typically', 'including', 'nation', '-', 'states', ',', 'and', 'capital', '##ism', '.', 'ana', '##r', '##chi', '##s', '##m', 'advocate', '##s', 'for', 'the', 'replacement', 'of', 'the', 'state', 'with', 'state', '##less', 'societies', 'and', 'vol', '##unt', '##ary', 'free', 'associations', '.', 'as', 'a', 'historically', 'left', '-', 'wing', 'movement', ',', 'this', 'reading', 'of', 'ana', '##r', '##chi', '##s', '##m', 'is', 'placed', 'on', 'the', 'far', '##th', '##est', 'left', 'of', 'the', 'political', 'spect', '##rum', ',', 'usually', 'described', 'as', 'the', 'libert', '##arian', 'wing', 'of', 'the', 'socialist', 'movement', '(', 'libert', '##arian', 'social', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens( tokenized_datasets[0][\"input_ids\"] )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import a pre-configured bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bert configuration is very heavy for the GPU, \n",
    "\n",
    "I arbitrary chose a lower specification, \n",
    "\n",
    "testing several combinations and used a binary search algorithm\n",
    "\n",
    "The best model (trainable with viable time) is {model2} with the specs specified below:\n",
    "\n",
    "- number of unique token (vocab_size) = 35_000\n",
    "\n",
    "- number of encoder layers concatenated (num_hidden_layers) = 4\n",
    "\n",
    "- number of multi-attention heads for each attention layer (num_attention_heads) = 8\n",
    "\n",
    "- size of the Feed-Foward-Layer (per token) (intermediate_size) = 1024\n",
    "\n",
    "- size of the embedding (hidden_size) = 256\n",
    "\n",
    "- max size for the sequence of token (max_position_embeddings) = 128\n",
    "\n",
    "- Activation function = GELU\n",
    "\n",
    "- Dropout probability = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 1024,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 128,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.45.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 35000\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diminish the specs, making is faster for my slow GPU\n",
    "\n",
    "config.vocab_size = vocabulary_size\n",
    "config.num_hidden_layers = 4\n",
    "config.num_attention_heads = 8\n",
    "config.intermediate_size = 1024\n",
    "config.hidden_size = 256\n",
    "config.max_position_embeddings = 128\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the Masked Language Modeling (MLM) task,\n",
    "\n",
    "the model is then connected to a layer of {vocabulary_size} and softmax function, to determine the masked tokens\n",
    "\n",
    "\n",
    "The BertForMaskedLM is a BERT model with the configuration passed connected to this \"head\" for the MLM task\n",
    "\n",
    "in total, with the configuration chosen, our model has ~12M parameters to train, which is reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:  12254136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(35000, 256, padding_idx=0)\n",
       "      (position_embeddings): Embedding(128, 256)\n",
       "      (token_type_embeddings): Embedding(2, 256)\n",
       "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-3): 4 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=256, out_features=35000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM(config=config)\n",
    "\n",
    "print(\"parameters: \", model.num_parameters())\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to mask the input randomly, we will use a data collator, which will randomly tokenize 15% of the input as [MASK] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# mask 15% of the tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm = True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the training arguments, \n",
    "\n",
    "- we will train the model passing though the dataset once (epoch == 1)\n",
    "\n",
    "- the batch size (number of inputs trained per iteration) is 32 (a middle ground for fast convergence and efficiency)\n",
    "\n",
    "- each 5000 iterations (steps) the model parameters will be locally saved\n",
    "\n",
    "- the number of saves or checkpoints saved simultaneusly is 2\n",
    "\n",
    "- it will used mixed precision of fp16, meaning that sometimes (when the precision won't affect the accuracy),\n",
    "it will not use the float of 32 bits and instead use float of 16 bits, speeding up the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='training/model2',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=5_000,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The past version (Model 1) was very bad in terms of perfomance and already used 5h to train.\n",
    "\n",
    "I forgot to save its specifications, but the loss score for this model was the ones below:\n",
    "\n",
    "    step 500: 10.345200\n",
    "\n",
    "    step 85000: 6.5\n",
    "    \n",
    "    step 200000: 6.331900\n",
    "\n",
    "It was due to very low configurations, around ~1M parameters. Therefore, model1 was no capable of storing and learn so much information.\n",
    "\n",
    "\n",
    "After this, I found a sweet spot and created (model 2), with a sufficient size so that it's capable of learning and storing enough information.\n",
    "\n",
    "But not so computationally expensive, so that it wouldn't be possible to train in my GTX 1070.\n",
    "\n",
    "It took exactly 10h 07min 06sec to train this second model !! \n",
    "\n",
    "for a total of 200245 steps\n",
    "\n",
    "After training, of course, save the model!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"trained/model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the self trained model and create a pipeline that will:\n",
    "\n",
    "- tokenize the input\n",
    " \n",
    "- push to the model\n",
    "\n",
    "- output the top 5 token that the model predict to fill each masked token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05094342678785324,\n",
       "  'token': 51,\n",
       "  'token_str': 'i',\n",
       "  'sequence': \"i didn ' t undestand the i, i will study harder.\"},\n",
       " {'score': 0.039161842316389084,\n",
       "  'token': 25433,\n",
       "  'token_str': 'world',\n",
       "  'sequence': \"i didn ' t undestand the world, i will study harder.\"},\n",
       " {'score': 0.01930907741189003,\n",
       "  'token': 25239,\n",
       "  'token_str': 'year',\n",
       "  'sequence': \"i didn ' t undestand the year, i will study harder.\"},\n",
       " {'score': 0.018123740330338478,\n",
       "  'token': 25851,\n",
       "  'token_str': 'name',\n",
       "  'sequence': \"i didn ' t undestand the name, i will study harder.\"},\n",
       " {'score': 0.01271581370383501,\n",
       "  'token': 25405,\n",
       "  'token_str': 'time',\n",
       "  'sequence': \"i didn ' t undestand the time, i will study harder.\"}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('trained/model2/')\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0\n",
    ")\n",
    "\n",
    "test1 = \"I didn't undestand the [MASK], I will study harder.\"\n",
    "\n",
    "fill_mask(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6839563250541687,\n",
       "  'token': 28583,\n",
       "  'token_str': 'girls',\n",
       "  'sequence': 'good girls, like bad girls'},\n",
       " {'score': 0.05937695503234863,\n",
       "  'token': 28930,\n",
       "  'token_str': 'boys',\n",
       "  'sequence': 'good girls, like bad boys'},\n",
       " {'score': 0.007571056485176086,\n",
       "  'token': 25701,\n",
       "  'token_str': 'women',\n",
       "  'sequence': 'good girls, like bad women'},\n",
       " {'score': 0.007524473592638969,\n",
       "  'token': 31362,\n",
       "  'token_str': 'saints',\n",
       "  'sequence': 'good girls, like bad saints'},\n",
       " {'score': 0.006937911733984947,\n",
       "  'token': 26545,\n",
       "  'token_str': '##ness',\n",
       "  'sequence': 'good girls, like badness'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Good girls, like bad [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.039969552308321,\n",
       "   'token': 25386,\n",
       "   'token_str': 'been',\n",
       "   'sequence': '[CLS] this sentence has been tokens, therefore it will output two [MASK] [SEP]'},\n",
       "  {'score': 0.03311655670404434,\n",
       "   'token': 43,\n",
       "   'token_str': 'a',\n",
       "   'sequence': '[CLS] this sentence has a tokens, therefore it will output two [MASK] [SEP]'},\n",
       "  {'score': 0.02915154956281185,\n",
       "   'token': 28750,\n",
       "   'token_str': 'shown',\n",
       "   'sequence': '[CLS] this sentence has shown tokens, therefore it will output two [MASK] [SEP]'},\n",
       "  {'score': 0.025615129619836807,\n",
       "   'token': 25189,\n",
       "   'token_str': 'not',\n",
       "   'sequence': '[CLS] this sentence has not tokens, therefore it will output two [MASK] [SEP]'},\n",
       "  {'score': 0.02202172763645649,\n",
       "   'token': 25320,\n",
       "   'token_str': 'two',\n",
       "   'sequence': '[CLS] this sentence has two tokens, therefore it will output two [MASK] [SEP]'}],\n",
       " [{'score': 0.06184706836938858,\n",
       "   'token': 28159,\n",
       "   'token_str': 'cases',\n",
       "   'sequence': '[CLS] this sentence has [MASK] tokens, therefore it will output two cases [SEP]'},\n",
       "  {'score': 0.040275562554597855,\n",
       "   'token': 33541,\n",
       "   'token_str': 'sentence',\n",
       "   'sequence': '[CLS] this sentence has [MASK] tokens, therefore it will output two sentence [SEP]'},\n",
       "  {'score': 0.02444721944630146,\n",
       "   'token': 25325,\n",
       "   'token_str': 'people',\n",
       "   'sequence': '[CLS] this sentence has [MASK] tokens, therefore it will output two people [SEP]'},\n",
       "  {'score': 0.017084723338484764,\n",
       "   'token': 28229,\n",
       "   'token_str': 'officers',\n",
       "   'sequence': '[CLS] this sentence has [MASK] tokens, therefore it will output two officers [SEP]'},\n",
       "  {'score': 0.01660514809191227,\n",
       "   'token': 31083,\n",
       "   'token_str': 'judges',\n",
       "   'sequence': '[CLS] this sentence has [MASK] tokens, therefore it will output two judges [SEP]'}]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"This sentence has [MASK] tokens, therefore it will output two [MASK]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for the NSP task\n",
    "\n",
    "I initially though that we would train the model for MLM task first, then load it again, and train for the NSP task\n",
    "\n",
    "But thinking better now, It will probably catastrophically forget a lot about MLM task when training NSP afterwards.\n",
    "\n",
    "Therefore, the best option would be train for both tasks simultaneously\n",
    "\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForNextSentencePrediction were not initialized from the model checkpoint at trained/model2 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForNextSentencePrediction, DataCollatorForLanguageModeling\n",
    "\n",
    "model = BertForNextSentencePrediction.from_pretrained(\"trained/model2\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
